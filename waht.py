import torch

prediction = torch. tensor([ 1.0388e+01,  8.2029e-01,  1.7874e+00,  3.0543e-01,  1.1235e-01,
         3.1823e-01,  1.2948e-01,  1.4879e-01,  2.0986e-03,  9.1391e-01,
         2.6320e-01,  3.7190e-02,  3.7773e-01,  1.1866e+01,  5.6596e-01,
         5.2105e+00,  1.5802e+00,  5.0950e-01,  4.3180e-01,  1.3654e+00,
         1.8127e-01,  2.0371e+00,  2.6523e+00,  1.9103e+00,  3.7291e+00,
         3.6546e+00,  6.1864e-01,  4.1105e-01,  3.6404e-01,  2.4985e-01,
        -8.6224e-03,  2.6482e-01,  9.7604e-02,  2.9288e+00,  5.5660e-01,
         5.5175e+00,  6.3764e-01,  3.8910e+00,  2.3590e-02,  1.0023e+00,
         6.6992e-01,  8.2438e-01,  2.6897e-02,  5.3092e-01,  6.7041e-02,
         9.7618e-02,  1.3775e-01,  8.5816e-03,  1.5808e+00,  2.7983e-01,
        -8.4102e-03,  3.6609e-02,  6.2121e-02,  3.2402e+00,  6.2004e-01,
         1.0415e+00,  9.7032e-01,  1.1566e+00,  1.5152e-02,  8.9547e-02,
         4.6651e-01,  1.8656e-01,  3.1480e+00,  1.6788e+01,  1.0745e+00,
         9.6605e-01,  1.2232e+00,  7.3130e-01,  1.6168e-01,  8.0061e-01,
         1.8522e-02,  1.7167e+00,  9.3311e-02,  6.5514e-01,  5.7743e+00,
         9.7886e+00,  1.4549e+00,  1.0766e-01,  2.8084e+00,  4.1639e-01,
         9.3449e-02,  2.5822e+01,  1.9512e+00,  7.7421e+00,  7.8994e+00,
         1.7316e-01,  7.3656e-01,  5.4593e-01,  3.6129e+00,  2.4028e-01,
         2.4501e-01,  6.4487e-03,  3.7636e-02, -5.1604e-05,  1.4459e-01,
         3.8757e-01,  3.0474e-03,  8.3580e+00,  7.3007e+00,  5.1498e+00,
         1.0405e+00, -3.1936e-02,  1.5331e-01,  1.0443e+01,  3.6613e-02,
         1.1857e-01,  1.0954e-01,  1.8237e+00,  5.5731e-02,  2.5360e+00,
         1.5141e-01,  3.1401e-01,  5.3194e-02,  4.8478e-01,  8.9365e-01,
         1.4990e+00,  2.1593e+00,  6.3726e-01,  5.6208e-01,  1.4230e+00,
         5.8818e+00,  1.1386e+00,  3.7827e-02,  2.3225e+00,  2.4296e+00,
         2.1079e+00, -4.1236e-02,  1.4625e-01,  2.3832e-01,  6.4507e-01,
         4.4606e-01,  1.5682e+00,  2.9309e+00,  8.1768e-02,  8.3632e-02,
         1.6462e-01,  1.1114e+01,  2.0433e+00,  1.6730e+00,  1.1825e+00,
         3.0346e-01,  3.3413e+00,  2.3639e-01,  1.5084e-01,  4.3429e-01,
         1.8479e-01,  2.6200e-01,  7.3342e-01,  2.1376e+00,  1.6705e+00,
         4.0186e+00,  1.8174e+00,  9.3196e-01,  3.8659e+00,  4.3240e-01,
         5.5511e-02,  1.5120e-01,  6.9665e-02,  1.7548e+00,  1.8385e+00,
         1.4726e+00,  1.6084e+00,  4.6848e-02,  3.2832e-02,  5.9086e-01,
         2.1557e-02,  1.8502e-01, -1.4788e-02,  1.6641e-02,  3.4065e-01,
         1.8637e+01,  3.0080e+00,  2.0234e-01,  4.1401e+00,  2.8401e-01,
         1.6985e-01,  4.8140e+00,  3.2934e-01,  1.5943e-02,  7.0406e-02,
         1.1359e-01,  2.9700e-01,  7.9197e-01,  1.0799e-01,  1.3888e+00,
         2.0607e+00,  1.3579e+01,  6.6439e-01,  2.9028e-02,  1.5792e+01,
         1.9980e-01,  3.4688e+00,  6.4033e-01,  2.4527e+00,  8.4389e-02,
         6.6765e-01,  1.4528e+00,  1.6141e+00,  5.0033e-01,  5.1467e+00,
         2.7063e+00,  2.5270e+00,  2.4201e+00,  5.9907e+00,  3.7028e-04,
         1.0381e-01,  4.1536e-02,  1.7453e+00,  2.5345e-01,  5.4541e-01,
         3.1324e+00,  3.7095e+00,  1.8224e+01,  1.3487e+00,  2.9781e+00,
         4.5800e-01,  1.7869e+00,  2.4730e+00,  1.4255e+00,  2.8697e-02,
         9.3353e-02,  1.8396e-01,  1.6427e-01,  4.8987e-01,  1.4998e+00,
         7.4052e-02,  4.0544e-01,  9.3706e-02,  2.4146e-02,  1.3022e+00,
         9.2403e-01,  2.0440e-01,  1.7002e+00,  2.8406e-01,  2.7682e-01,
         1.3983e+00,  1.6497e+00,  1.8236e-01,  1.8183e+00,  2.1003e-02,
         3.9371e+00,  1.2911e+00,  8.2287e-01,  2.4612e-01,  3.4412e+00,
         2.4238e+00,  5.1058e-01,  4.5463e+00,  7.5123e-03,  2.4352e+00,
         5.5337e-01,  2.5363e+00,  2.4599e+00,  8.3512e+00,  9.8103e-01,
         3.2419e-01,  7.1714e-01,  3.5974e-01,  1.0121e-02,  1.7631e-02,
         1.7315e+00,  1.0973e+00,  3.9314e+00,  1.2484e+00,  6.2171e-01,
         1.9252e-01,  5.6119e+00,  1.8766e+00,  9.8258e-01,  4.5340e-02,
         2.9178e+00,  4.2210e+00,  3.9343e-01,  5.8654e-01,  7.4664e-01,
         4.7975e-01,  3.0438e+00,  1.0816e+00,  1.6650e+00,  1.3884e-01,
         1.5049e+00,  3.4288e-01,  1.1799e+00,  6.6568e-01,  1.4240e+00,
         1.9776e-01,  2.0881e+00,  2.1635e-01,  4.2612e-03,  2.9945e+00,
         1.8516e+01,  5.9219e-01,  4.4562e-01,  1.3937e-02,  1.9699e-01,
         5.0517e-02,  1.3936e+00,  3.8983e-01,  7.4418e-03,  1.7141e+00,
         7.0582e+00,  7.5047e-02,  1.3985e+00,  6.9069e-02, -2.3238e-03,
         2.8272e+01, -7.7735e-03,  1.7006e-02,  9.8068e-02,  1.1971e-01,
         2.6836e-01,  1.2023e+00,  1.7332e+00,  1.5057e+00,  1.4564e+01,
         1.3915e+00,  1.2182e+00,  7.9150e-01,  1.0167e-01,  1.7201e+00,
         1.6598e-01,  2.5055e-02,  1.1912e-01,  1.7168e+00,  2.2890e+00,
         1.1333e-01,  2.3071e+00,  1.2993e+00,  4.4654e-01, -1.4552e-02,
         1.6615e+00,  1.4013e+00,  4.1226e-01,  2.1021e-01,  3.0884e-02,
         8.6409e-01,  1.8906e-01,  1.0917e-01,  1.6760e+00,  1.0745e-01,
         2.2388e-01,  7.3473e-02,  5.4523e-01,  4.0990e+00,  8.5253e-02,
         7.5874e-01,  8.4988e-01], device='cuda:0')
act=torch.tensor([16.,  0.,  5.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0., 14.,
         0.,  4.,  2.,  1.,  0.,  2.,  0.,  3.,  5.,  4.,  5.,  3.,  0.,  0.,
         0.,  1.,  0.,  2.,  0.,  3.,  0.,  4.,  0.,  4.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  1.,  0.,  2.,  0.,  0.,  0.,  0.,  3.,  0.,  3.,
         0.,  3.,  0.,  0.,  1.,  0.,  1., 20.,  1.,  2.,  0.,  0.,  0.,  1.,
         0.,  3.,  0.,  0., 11., 10.,  7.,  0.,  1.,  0.,  0., 37.,  2.,  6.,
         3.,  0.,  0.,  0., 11.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,
         5.,  1.,  2.,  0.,  0.,  5.,  0.,  0.,  0.,  0.,  0.,  3.,  0.,  0.,
         0.,  0.,  0.,  1.,  3.,  0.,  0.,  0.,  4.,  0.,  0.,  0.,  0.,  5.,
         0.,  0.,  0.,  1.,  0.,  0.,  2.,  0.,  0.,  0., 14.,  0.,  0.,  2.,
         0., 12.,  0.,  0.,  0.,  0.,  0.,  0.,  4.,  2., 12.,  2.,  1., 10.,
         1.,  0.,  0.,  0.,  0.,  3.,  4.,  2.,  0.,  0.,  3.,  0.,  0.,  0.,
         0.,  0., 13.,  0.,  0.,  5.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  1.,  4.,  9.,  0.,  0., 28.,  1.,  1.,  0.,  1.,  0.,  0.,
         8.,  2., 23.,  5.,  0.,  0.,  2.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  1.,  0.,  0.,  1.,  0.,
         1.,  0.,  2.,  1.,  6.,  0.,  0.,  1.,  0.,  6.,  0.,  0.,  0.,  0.,
         2.,  7.,  2.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  5.,  1.,  0.,  0.,
         2.,  7.,  0.,  0.,  1., 18.,  0.,  0.,  0.,  0.,  0.,  0.,  3.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 11.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  3., 10.,  0.,  4.,  0.,  0., 32.,  0.,  0.,
         0.,  1.,  0.,  2.,  0.,  3., 17., 12.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  1.,  1.,  0.,  0.,  3.,  1.,  0.,  3.,  2.,  0.,  0.,  0.,  0.,
         0.,  0.,  1.,  0.,  0.,  0.,  0., 10.,  0.,  3.,  2.])
from scipy.stats import pearsonr
# 使用clamp函数将负数值规整为0
print(act.shape)
clamped_prediction = torch.clamp(prediction, min=0)

# 对张量中的所有值进行四舍五入
rounded_prediction = torch.round(clamped_prediction)

# 将结果从GPU转移到CPU，并转换为numpy数组以便查看
rounded_prediction_cpu = rounded_prediction.cpu().numpy()
print(rounded_prediction_cpu)
#pcc, _ = pearsonr(rounded_prediction_cpu, act)
#print(f"Pearson Correlation Coefficient: {pcc}")